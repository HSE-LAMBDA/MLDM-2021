{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVQ9xy45yDT2"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2021/blob/master/10-architectures/MLDM_2021_seminar10_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0YKWq16yDT4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import skimage\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage.filters import convolve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.random.set_seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx9NudPkyDT5"
   },
   "source": [
    "In this notebook we are going to use a dataset with [annotated images of bees](https://www.kaggle.com/jenny18/honey-bee-annotated-images) from various locations of US, captured over several months during 2018, at different hours, from various bees subspecies, and with different health problems. We will try to solve classification problem using [pretrained model and fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning).\n",
    "\n",
    "> The original batch of images was extracted from still time-lapse videos of bees. By averaging the frames to calculate a background image, each frame of the video was subtracted against that background to bring out the bees in the forefront. The bees were then cropped out of the frame so that each image has only one bee. Because each video is accompanied by a form with information about the bees and hive, the labeling process is semi-automated. Each video results in differing image crop quality levels. This dataset will be updated as more videos and data become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpgRh-mUyDT6"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeBj-gVayDT6"
   },
   "source": [
    "Let's quickly recap key points about the data we are doing to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUo5lZxzyDT6"
   },
   "source": [
    "The data contains the following values:\n",
    "\n",
    "* file - the image file name;\n",
    "* date - the date when the picture was taken;\n",
    "* time - the time when the picture was taken;\n",
    "* location - the US location, with city, state and country names;\n",
    "* zip code - the ZIP code associated with the location;\n",
    "* subspecies - the subspecies to whom the bee in the current image belongs;\n",
    "* health - this is the health state of the bee in the current image;\n",
    "* pollen_carrying - indicates if the picture shows the bee with pollen attached to the legs;\n",
    "* caste - the bee caste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV0DafE7yDT7",
    "outputId": "2c9e6df5-ddb6-48c3-e017-47ec9b61d7a3"
   },
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/HSE-LAMBDA/MLDM-2021/main/09-convolutions-and-regularization/bees_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFYhCTMmyDT7"
   },
   "outputs": [],
   "source": [
    "#!unzip bees_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGGmJCxoyDT8"
   },
   "outputs": [],
   "source": [
    "bees=pd.read_csv('bee_data.csv', index_col=False, dtype={'subspecies':'category', 'health':'category','caste':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M7x27QuZyDT8",
    "outputId": "3cdffe17-f727-4b93-8c6a-72a2f911d9d5"
   },
   "outputs": [],
   "source": [
    "bees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtkBeTtofq_e"
   },
   "outputs": [],
   "source": [
    "bees['is_healty'] = (bees['health'] == 'healthy').map({True:'healthy', False: 'unhealthy'}).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcmNGST5l4h2"
   },
   "source": [
    "Pick up a target column to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slBLzBEdgYaN"
   },
   "outputs": [],
   "source": [
    "target_col = 'is_healty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rloRXrRmyDT8"
   },
   "outputs": [],
   "source": [
    "# Check whether image for some particular given discription exists\n",
    "img_exists = bees['file'].apply(lambda f: os.path.exists('bee_imgs/bee_imgs/' + f))\n",
    "bees = bees[img_exists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toDP7ZhvyDT9"
   },
   "source": [
    "As you may remember, we need do balance our class-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnYgtipVyDT-"
   },
   "outputs": [],
   "source": [
    "def split_balance(bees, target):\n",
    "    # Split to train and test before balancing\n",
    "    train_bees, test_bees = train_test_split(bees, random_state=24)\n",
    "\n",
    "    # Split train to train and validation datasets\n",
    "    train_bees, val_bees = train_test_split(train_bees, test_size=0.1, random_state=24)\n",
    "\n",
    "    #Balance by subspecies to train_bees_bal_ss dataset\n",
    "    # Number of samples in each category\n",
    "    ncat_bal = int(len(train_bees)/train_bees[target].cat.categories.size)\n",
    "    train_bees_bal = train_bees.groupby(target, as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True)).reset_index(drop=True)\n",
    "    return(train_bees_bal, val_bees, test_bees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoCi27bIyDT_"
   },
   "outputs": [],
   "source": [
    "train_bees_bal, val_bees, test_bees = split_balance(bees, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s81e71jyyDT_"
   },
   "source": [
    "# Subspecies classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy6S9hEwyDT_"
   },
   "outputs": [],
   "source": [
    "# Some default network parameters\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 96, 96\n",
    "KERNEL_SIZE = 3\n",
    "IMAGE_CHANNELS = 3\n",
    "RANDOM_STATE = 1337\n",
    "N_EPOCH = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDFehzQ8yDT_"
   },
   "source": [
    "Here you can find a few auxiliary functions that we will help us through the model-building procedure. The dataset contains images of different shapes. The function below helps us to read images from the image-files and scale all images to IMAGE_WIDTH x IMAGE_HEIGHT x IMAGE_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fF-lcMujyDT_"
   },
   "outputs": [],
   "source": [
    "def read_img(file, img_folder='bee_imgs/bee_imgs/'):    \n",
    "    img = skimage.io.imread(img_folder + file)\n",
    "    img = skimage.transform.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), mode='reflect')\n",
    "    return img[:,:,:IMAGE_CHANNELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW-L1BfWyDUA"
   },
   "source": [
    "`tf.keras.preprocessing.image.ImageDataGenerator` may help us to generate batches of tensor-image-data with [real-time data augmentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Z38qMkgyDUA"
   },
   "outputs": [],
   "source": [
    "def prepare2train(train_bees, val_bees, test_bees, target):\n",
    "    # Bees already splitted to train, validation and test\n",
    "    # Load and transform images to have equal width/height/channels. \n",
    "    # read_img function is defined to use in both health and subspecies. \n",
    "    # Use np.stack to get NumPy array for CNN input\n",
    "\n",
    "    # Train data\n",
    "    train_X = np.stack(train_bees['file'].apply(read_img))\n",
    "    train_y  = pd.get_dummies(train_bees[target], drop_first=False)\n",
    "\n",
    "    # Validation during training data to calculate val_loss metric\n",
    "    val_X = np.stack(val_bees['file'].apply(read_img))\n",
    "    val_y = pd.get_dummies(val_bees[target], drop_first=False)\n",
    "\n",
    "    # Test data\n",
    "    test_X = np.stack(test_bees['file'].apply(read_img))\n",
    "    test_y = pd.get_dummies(test_bees[target], drop_first=False)\n",
    "\n",
    "    # Data augmentation - a little bit rotate, zoom and shift input images.\n",
    "    generator = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.1, # Randomly zoom image \n",
    "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False)\n",
    "    generator.fit(train_X)\n",
    "    return (generator, train_X, val_X, test_X, train_y, val_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy3C0ZWEyDUA"
   },
   "outputs": [],
   "source": [
    "generator, train_X, val_X, test_X, train_y, val_y, test_y = prepare2train(train_bees_bal, val_bees, test_bees, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYIG_wI7plhH"
   },
   "source": [
    "# Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG9qA2domI4S"
   },
   "source": [
    "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n",
    "\n",
    "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o1qYLdVmXCX"
   },
   "source": [
    "Let's pick the MobileNet V2 model as a base model. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. \n",
    "The very last classification layer is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\", retaining more generality as compared to the final/top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SurQ_zRFyxYs"
   },
   "outputs": [],
   "source": [
    "# Instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet.\n",
    "# Load a network that doesn't include the classification layers at the top, which is ideal for feature extraction. \n",
    "IMG_SHAPE = (IMAGE_WIDTH, IMAGE_WIDTH, 3)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3Mm96rWzJPU",
    "outputId": "8d094cd0-b3e9-4af0-a5a6-abb218a44956"
   },
   "outputs": [],
   "source": [
    "# This feature extractor converts each image into a 3x3x1280 block of features.\n",
    "image_batch, label_batch = next(generator.flow(train_X,train_y, batch_size=BATCH_SIZE))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv24ImmYolff"
   },
   "source": [
    "It is important to freeze the convolutional base before you compile and train the model. Freezing prevents the weights in a given layer from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0ksp9vr18nz"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9KE-TBmo1yu"
   },
   "source": [
    "To generate predictions from the block of features we may use a `GlobalAveragePooling2D` layer to convert the features to a single 1280-element vector per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YWTqzW518W-",
    "outputId": "61cc1872-cdd4-4935-c3df-16dd5568ad18"
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkvWtWSWpMgH"
   },
   "source": [
    "`Dense` layer converts these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yqz2ev4U18Nz",
    "outputId": "c68530c7-d68e-4b85-be6a-7c1b97986e52"
   },
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(train_y.columns.size)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz4AugiG4_U1"
   },
   "outputs": [],
   "source": [
    "# Freeze all the layers\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "# Check the trainable status of the individual layers\n",
    "# for layer in base_model.layers:\n",
    "#     print(layer, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPH7_x86RYYX"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
    "x = base_model(inputs, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model1 = tf.keras.Model(inputs, outputs)\n",
    "base_learning_rate = 0.01\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CH6uXS6GjMtg",
    "outputId": "558a3db0-f397-4f49-fb74-d606699c814d"
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsI1c5b859No"
   },
   "outputs": [],
   "source": [
    "# We'll stop training if no improvement after some epochs\n",
    "earlystopper1 = keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "\n",
    "# Save the best model during the traning\n",
    "checkpointer1 = keras.callbacks.ModelCheckpoint('best_model1.h5',\n",
    "                                                monitor='val_accuracy',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev5xGLc359IY",
    "outputId": "4fabd02d-be93-435d-bb2f-b70d1768497c"
   },
   "outputs": [],
   "source": [
    "training1 = model1.fit(generator.flow(train_X,train_y, batch_size=BATCH_SIZE),\n",
    "                                 epochs=N_EPOCH,\n",
    "                                 validation_data=[val_X, val_y],\n",
    "                                 steps_per_epoch=40,\n",
    "                                 callbacks=[earlystopper1, checkpointer1])\n",
    "# To get the best saved weights\n",
    "#model1.load_weights('best_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dOInGB7d8q-"
   },
   "outputs": [],
   "source": [
    "def eval_model(training, model, test_X, test_y, target):\n",
    "    \n",
    "    ## Trained model analysis and evaluation\n",
    "    f, ax = plt.subplots(2,1, figsize=(5,5))\n",
    "    ax[0].plot(training.history['loss'], label=\"Loss\")\n",
    "    ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n",
    "    ax[0].set_title('%s: loss' % target)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    ax[1].plot(training1.history['accuracy'], label=\"Accuracy\")\n",
    "    ax[1].plot(training1.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "    ax[1].set_title('%s: accuracy' % target)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    test_pred = model.predict(test_X)    \n",
    "    print(\"Classification report\")\n",
    "    test_pred = np.argmax(test_pred, axis=1)\n",
    "    test_truth = np.argmax(test_y.values, axis=1)\n",
    "    print(classification_report(test_truth, test_pred))\n",
    "\n",
    "    # Loss function and accuracy\n",
    "    test_res = model.evaluate(test_X, test_y.values, verbose=0)\n",
    "    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "4qL_unB97Dyf",
    "outputId": "e648ccfc-bcc1-4b8f-fd2e-c2679ede5f10"
   },
   "outputs": [],
   "source": [
    "eval_model(training1, model1, test_X, test_y, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ju9fZwVpxc9"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX6kC6S5p4Vg"
   },
   "source": [
    "In the feature extraction experiment, we were only training a few layers on top of the base model. The weights of the pre-trained network were not updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLfx-8ZPjXP1",
    "outputId": "f9188e03-92e7-4933-b743-40bb748b8258"
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xL1ECVPqE_t"
   },
   "source": [
    "This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaMBEJt_7X7T"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j443RHqE7X2Y",
    "outputId": "bb6a033b-f091-4e8f-adf5-c9b3cd1e6bdb"
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 140\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feqOWbLsTK7D",
    "outputId": "23a00309-49a7-405a-a8ec-cd0df9deb36e"
   },
   "outputs": [],
   "source": [
    "len(model1.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpjivElfqUA1"
   },
   "source": [
    "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUxaRcRmQycn"
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/500), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "derElfNWh-Bb",
    "outputId": "8196d908-159d-47fc-d3a0-3b93579674dc"
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOgIDAXB832z"
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "total_epochs = N_EPOCH + fine_tune_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6MAV2uMUeSe"
   },
   "outputs": [],
   "source": [
    "# # Get the best saved weights\n",
    "# model1.load_weights('best_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JggdUIX8_zQ",
    "outputId": "625c2a71-ce0b-4856-b01e-a059e25c7052"
   },
   "outputs": [],
   "source": [
    "training1_fine = model1.fit_generator(generator.flow(train_X,train_y, batch_size=BATCH_SIZE),\n",
    "                                 epochs=total_epochs,\n",
    "                                 initial_epoch=training1.epoch[-1],\n",
    "                                 validation_data=[val_X, val_y],\n",
    "                                 steps_per_epoch=40,\n",
    "                                 callbacks=[earlystopper1, checkpointer1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9Jxo325q3R9"
   },
   "source": [
    "In case your quality decreases, try to use a lower value for the learning rate and play around the number of unfreezed layers in your base model. Solving other problems you may get some overfitting as the new training set is relatively small or similar to the original dataset that your base model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "vYI1fcmmCt6r",
    "outputId": "a92bd298-db8d-47ac-8b34-5730f05de874"
   },
   "outputs": [],
   "source": [
    "eval_model(training1_fine, model1, test_X, test_y, 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYEkb8ncNJ5L"
   },
   "outputs": [],
   "source": [
    "#model1.load_weights('best_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIUg_uAWr1PW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLDM_2021_seminar10_fine_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
