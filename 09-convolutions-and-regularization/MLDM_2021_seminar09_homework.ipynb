{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2021/blob/master/09-convolutions-and-regularization/MLDM_2021_seminar09_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un3T_AERfRG4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BphdyxaWjbWp"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_train = tfds.load(name=\"fashion_mnist\", split=\"train\").prefetch(60000).cache()\n",
    "data_test  = tfds.load(name=\"fashion_mnist\", split=\"test\" ).prefetch(10000).cache()\n",
    "\n",
    "# Array for decoding the categories\n",
    "label_names = np.array(['T-shirt/top',\n",
    "                        'Trouser',\n",
    "                        'Pullover',\n",
    "                        'Dress',\n",
    "                        'Coat',\n",
    "                        'Sandal',\n",
    "                        'Shirt',\n",
    "                        'Sneaker',\n",
    "                        'Bag',\n",
    "                        'Ankle boot'])\n",
    "\n",
    "# Get a single data batch of 25 images\n",
    "sample_data = next(iter(data_train.batch(25)))\n",
    "sample_images = sample_data['image']\n",
    "sample_labels = sample_data['label']\n",
    "\n",
    "# Plot the images in a 5x5 grid\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(\n",
    "    sample_images.numpy().reshape(5, 5, 28, 28).transpose((0, 2, 1, 3)).reshape(140, 140),\n",
    "    cmap='gray'\n",
    ")\n",
    "# Print corresponding labels\n",
    "print(label_names[sample_labels.numpy().reshape(5, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6K2SAJdkQHm"
   },
   "source": [
    "# Task 1 (5 points + 1 point for the short comment)\n",
    "\n",
    "Fill the gaps below to build a convolutional neural network and classify the images. Write a short comment on the validation metric images that you should obtain in the last cell.\n",
    "\n",
    "Some hints for classes:\n",
    " - `tf.keras.layers.Conv2D` - convolutional layer\n",
    " - `tf.keras.layers.MaxPool2D` - maxpool layer\n",
    " - `tf.keras.layers.BatchNormalization` - batchnorm layer\n",
    " - `tf.keras.layers.Dropout` - dropout layer\n",
    " - `tf.keras.layers.Reshape` - reshaping layer (to convert the image-like representation to a vector-like representation deep down in the network\n",
    "\n",
    "Try to follow the general deep convolutional architecture:\n",
    " - combine convolutions with maxpoolings to reduce the spacial size of the representation\n",
    " - increase the number of filters as you go deeper\n",
    " - when the spacial size of your representation is small enough (1-2 pixels), convert (reshape) it to a vector and then use fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpJ6rBBIXZPt"
   },
   "source": [
    "As you do this task, try to answer the following questions to yourself:\n",
    " - should I place batchnorm before or after the activation function?\n",
    "  - (to answer this one, think how inactive neurons would affect the batchnorm statistics)\n",
    " - should I add dropout before or after batchnorm?\n",
    "  - (think how batchnorm and dropout might interfere)\n",
    " - do I need an activation for the output layer?\n",
    "  - (check the loss function used)\n",
    " - does it make sense to add a dropout to the output layer?\n",
    "  - (common sense)\n",
    " - is it a good idea to add a batchnorm to the output layer?\n",
    "  - (in fact, I don't have a good answer to this one, but imo a batchnorm in the last layer might lead to weird effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir0hafMrjgNY"
   },
   "outputs": [],
   "source": [
    "def build_model(use_batchnorm=False, dropout_rate=0.):\n",
    "  \"\"\"\n",
    "  Fill in the layers below.\n",
    "\n",
    "  If use_batchnorm is True, add a batchnorm layer to **every** convolution and\n",
    "  dense layer (except for the output one).\n",
    "  If dropout_rate > 0, add a dropout layer with `rate=dropout_rate` to **every**\n",
    "  convolution and dense layer (except for the output one).\n",
    "  \"\"\"\n",
    "  layers = []\n",
    "\n",
    "  layers.append(<YOUR CODE>)\n",
    "  if use_batchnorm: layers.append(<YOUR CODE>)\n",
    "  <YOUR CODE>\n",
    "\n",
    "  model = tf.keras.Sequential(layers)\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHG2vJcDakw5"
   },
   "source": [
    "The code below creates and trains a bunch of models, then plots their validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXrE2XSaEAwE"
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "  dict(use_batchnorm=False, dropout_rate=0),\n",
    "  dict(use_batchnorm=False, dropout_rate=0.01),\n",
    "  dict(use_batchnorm=False, dropout_rate=0.05),\n",
    "  dict(use_batchnorm=False, dropout_rate=0.5),\n",
    "  dict(use_batchnorm=True, dropout_rate=0),\n",
    "  dict(use_batchnorm=True, dropout_rate=0.01),\n",
    "  dict(use_batchnorm=True, dropout_rate=0.05),\n",
    "  dict(use_batchnorm=True, dropout_rate=0.5),\n",
    "]\n",
    "\n",
    "models = {str(config) : build_model(**config) for config in configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pxRcezOILTn"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "def preprocess(x):\n",
    "  return (tf.cast(x['image'], 'float32') / 255., x['label'])\n",
    "\n",
    "for config, model in models.items():\n",
    "  print(\"Working on model:\", config)\n",
    "  model.fit(x=data_train.map(preprocess).shuffle(60000).batch(batch_size), epochs=10,\n",
    "            validation_data=data_test.map(preprocess).batch(4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmAqM7dbIa48"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8), dpi=100)\n",
    "color_cycle = iter(plt.rcParams['axes.prop_cycle'])\n",
    "\n",
    "colors = {}\n",
    "\n",
    "lines = []\n",
    "labels = []\n",
    "for config, model in models.items():\n",
    "  config = eval(config)\n",
    "  if config['dropout_rate'] not in colors:\n",
    "    colors[config['dropout_rate']] = next(color_cycle)\n",
    "\n",
    "  color = colors[config['dropout_rate']]['color']\n",
    "\n",
    "  style = '-' if config['use_batchnorm'] else '--'\n",
    "  line, = plt.plot(model.history.history['val_sparse_categorical_accuracy'], style,\n",
    "                   c=color)\n",
    "  \n",
    "  if config['use_batchnorm']:\n",
    "    lines.append(line)\n",
    "    labels.append(f\"do_rate = {config['dropout_rate']}\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"test accuracy\");\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines += [Line2D([0], [0], linestyle='-', color='w'),\n",
    "          Line2D([0], [0], linestyle='-', color='k'),\n",
    "          Line2D([0], [0], linestyle='--', color='k')]\n",
    "labels += ['', 'batchnorm on', 'batchnorm off']\n",
    "plt.legend(lines, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NluyJb7Sbaas"
   },
   "source": [
    "\\<YOUR SHORT COMMENT GOES HERE\\>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7ZGktFxlJ/mGBT3oblECD",
   "include_colab_link": true,
   "name": "MLDM-2020-seminar09-homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
