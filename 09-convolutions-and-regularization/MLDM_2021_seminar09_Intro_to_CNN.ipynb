{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2021/blob/master/09-convolutions-and-regularization/MLDM_2021_seminar09_Intro_to_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9YRgxPNxoR5"
   },
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjAm7BDByib1"
   },
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/HSE-LAMBDA/MLDM-2021/main/09-convolutions-and-regularization/img.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3w6ZXzJt8KL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLX7yImcCwF7"
   },
   "source": [
    "## Demonstration: convolving to extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6FoDIUQyv0I"
   },
   "source": [
    "Let's check out the image we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOatDoQmBC9r"
   },
   "outputs": [],
   "source": [
    "img = np.load(\"img.npy\")\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDSkiUcsyyWT"
   },
   "source": [
    "At first, we'll experiment with `tf.nn.conv2d` - the function that performs 2d image convolution.\n",
    "\n",
    "*Note:* this function is designed to work in the context of a neural network (i.e. where input and output come in batches and have multiple channels), so the functin expects 4D tensors rather than 2D. We'll write a short wrapper to work with 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqxS91NrzmCU"
   },
   "outputs": [],
   "source": [
    "def convolve(img, kernel):\n",
    "  return tf.nn.conv2d(\n",
    "      img[None,...,None],\n",
    "      kernel[...,None,None], strides=1, padding='VALID'\n",
    "    ).numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUNG0CawzszI"
   },
   "source": [
    "Let's try some simple kernels extracting horizontal and vertical edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZpMpzar2OQ8"
   },
   "outputs": [],
   "source": [
    "kernel_ver_edge = tf.convert_to_tensor(\n",
    "    [[ 1., -1.],\n",
    "     [ 1., -1.]]\n",
    ")\n",
    "kernel_hor_edge = tf.convert_to_tensor(\n",
    "    [[ 1.,  1.],\n",
    "     [-1., -1.]]\n",
    ")\n",
    "\n",
    "vertical_edges = convolve(img, kernel_ver_edge)\n",
    "horizontal_edges = convolve(img, kernel_hor_edge)\n",
    "\n",
    "plt.figure(figsize=(4, 5), dpi=150)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(vertical_edges);\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(horizontal_edges);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPKISCxqz1GS"
   },
   "source": [
    "We can combine the result, e.g. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq8nOprW3_6g"
   },
   "outputs": [],
   "source": [
    "edges = (vertical_edges**2 + horizontal_edges**2)**0.5\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(edges);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3HliLWlz6_Z"
   },
   "source": [
    "Another example, blurring kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AIGeOsi_BAl"
   },
   "outputs": [],
   "source": [
    "kernel_blur = tf.convert_to_tensor([[1.,  4.,  7.,  4., 1.],\n",
    "                                    [4., 16., 26., 16., 4.],\n",
    "                                    [7., 26., 41., 26., 7.],\n",
    "                                    [4., 16., 26., 16., 4.],\n",
    "                                    [1.,  4.,  7.,  4., 1.]]) / 273\n",
    "\n",
    "edges_blurred = convolve(edges, kernel_blur)\n",
    "\n",
    "### Uncomment these lines one by one to see the effect\n",
    "### gradually increasing:\n",
    "# edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "# edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "# edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "# edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "### Keep them **uncommented** for the further code to work\n",
    "\n",
    "plt.imshow(edges_blurred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X770Mh2A0Wqb"
   },
   "source": [
    "Let's pick up a small patch out of this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X-vXBPF4EW0"
   },
   "outputs": [],
   "source": [
    "edges_subset = edges_blurred[210:243, 246:282]\n",
    "plt.imshow(edges_subset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro9iN5up0aHg"
   },
   "source": [
    "What do you think will happen if we use this patch as a kernel when running convolution on the edges image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvZ_X8ES702i"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(convolve(edges_blurred, edges_subset))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL9bNU1s0kCj"
   },
   "source": [
    "Note how this kernel highlighted the location of that shape on the input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vIYgMUDDDtl"
   },
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ry0hAdre05d4"
   },
   "source": [
    "Keras has predefined convolutional layers that make use of the convolution function described above.\n",
    "\n",
    "Note that in the context of deep learning the convolutional kernel is **trainable**, i.e. the network tries to find the best kernel to extract useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShVolbP9AP6O"
   },
   "outputs": [],
   "source": [
    "# Let's build a layer that takes an image with a single channel and outputs \n",
    "# two-channel feature representation:\n",
    "conv_layer = tf.keras.layers.Conv2D(\n",
    "    filters=2, kernel_size=2)\n",
    "conv_layer.build(input_shape=(None, None, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpCn2kLb1hGI"
   },
   "source": [
    "Note that the kernel is initialized randomly (for optimization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Elq467gqB-vU"
   },
   "outputs": [],
   "source": [
    "conv_layer.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILl2tmYR1nTE"
   },
   "source": [
    "but we can set it to e.g. our edge detecting kernel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ri17z9UDCJRl"
   },
   "outputs": [],
   "source": [
    "conv_layer.kernel[..., 0, 0].assign(kernel_hor_edge)\n",
    "conv_layer.kernel[..., 0, 1].assign(kernel_ver_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJfbMQte1uNV"
   },
   "source": [
    "And now the layer performs exactly the same edge-detecting operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MgArqZ7CWss"
   },
   "outputs": [],
   "source": [
    "# Note how we add the batch and channel dimensions here\n",
    "result = conv_layer(img[None,...,None].astype('float32')).numpy().squeeze()\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(result[...,0])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(result[...,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofjxSPFFDtc9"
   },
   "source": [
    "## Ridiculously impractical example: trying to learn the kernels from the 1st demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1_JL4qf3X9y"
   },
   "source": [
    "Let's make a keras model that make a similar transformation to the one we did above (i.e. edge detection + blur). We'll try to learn corresponding kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_RhBYbaEK2B"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "      # a block to \"reproduce\" edge detection:\n",
    "      tf.keras.layers.Conv2D(filters=2, kernel_size=2, activation='elu'),\n",
    " \n",
    "      tf.keras.layers.Conv2D(filters=100, kernel_size=1, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=1, kernel_size=1, activation='elu'),\n",
    "\n",
    "      # a block to \"reproduce\" blurring\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='elu'),\n",
    "      tf.keras.layers.Conv2D(filters=1, kernel_size=3, activation='elu'),\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=(None, None, None, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B85zCOvh4Jnr"
   },
   "source": [
    "Note: we have quite a lot of parameters and just a single image - we'll probably overfit heavily..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKZqE80NFMcp"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "opt = tf.optimizers.Adam()\n",
    "\n",
    "loss_values = []\n",
    "for _ in trange(500):\n",
    "  with tf.GradientTape() as t:\n",
    "    prediction = model(img[None,...,None].astype('float32'))\n",
    "    loss = tf.reduce_mean((prediction - edges_blurred[None,...,None])**2)\n",
    "  grads = t.gradient(loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  loss_values.append(loss.numpy())\n",
    "\n",
    "plt.plot(loss_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4Eg2NzB4aDi"
   },
   "source": [
    "Let's have a look on what the result of our model's transformation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jabJeGT-GwFm"
   },
   "outputs": [],
   "source": [
    "plt.imshow(model(img[None,...,None].astype('float32')).numpy().squeeze());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heSFA7kB4obd"
   },
   "source": [
    "Try checking the following things:\n",
    " - Do the first layers indeed extract the edges?\n",
    " - What the intermediate representations of our model look like? (e.g. take the input and only apply a subset of layers from our model to it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJrOLZB75emu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7E7AmG4/5Ir7Qz1fFRzb0",
   "include_colab_link": true,
   "name": "MLDM-2021-seminar09-Intro-to-CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
