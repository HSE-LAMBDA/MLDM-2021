{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2021/blob/master/09-convolutions-and-regularization/MLDM_2021_seminar09_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import skimage\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.ndimage.filters import convolve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to explore a dataset with [annotated images of bees](https://www.kaggle.com/jenny18/honey-bee-annotated-images) from various locations of US, captured over several months during 2018, at different hours, from various bees subspecies, and with different health problems.\n",
    "\n",
    "> The original batch of images was extracted from still time-lapse videos of bees. By averaging the frames to calculate a background image, each frame of the video was subtracted against that background to bring out the bees in the forefront. The bees were then cropped out of the frame so that each image has only one bee. Because each video is accompanied by a form with information about the bees and hive, the labeling process is semi-automated. Each video results in differing image crop quality levels. This dataset will be updated as more videos and data become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from an Exploratory Data Analysis (EDA) where we highlight labels' distributions. We will try to perform subspecies classification through the notebook, but you may try use \"health\" as a target as well. We are going to work with images only, however you may try to use other features as well, so you may find a way more [detailed EDA in the notebook](https://www.kaggle.com/gpreda/honey-bee-subspecies-classification/notebook), that is used as a baseline for this seminar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the following values:\n",
    "\n",
    "* file - the image file name;\n",
    "* date - the date when the picture was taken;\n",
    "* time - the time when the picture was taken;\n",
    "* location - the US location, with city, state and country names;\n",
    "* zip code - the ZIP code associated with the location;\n",
    "* subspecies - the subspecies to whom the bee in the current image belongs;\n",
    "* health - this is the health state of the bee in the current image;\n",
    "* pollen_carrying - indicates if the picture shows the bee with pollen attached to the legs;\n",
    "* caste - the bee caste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/HSE-LAMBDA/MLDM-2021/main/09-convolutions-and-regularization/bees_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip bees_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bees=pd.read_csv('bee_data.csv', index_col=False, dtype={'subspecies':'category', 'health':'category','caste':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether image for some particular given discription exists\n",
    "img_exists = bees['file'].apply(lambda f: os.path.exists('bee_imgs/bee_imgs/' + f))\n",
    "bees = bees[img_exists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distributions of bees by categories that we are going to use as our target labels and have a look at the images. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "\n",
    "#Plot a bar or pie chart of Subspecies labels\n",
    "<YOUR CODE>\n",
    "ax[0].set_title('Subspecies')\n",
    "\n",
    "#Same plot fot the health label\n",
    "<YOUR CODE>\n",
    "ax[1].set_title('Health')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'bee_imgs/bee_imgs/'\n",
    "\n",
    "f_figsize = (14,3)\n",
    "\n",
    "subspecies = bees['subspecies'].cat.categories\n",
    "f, ax = plt.subplots(nrows=1,ncols=subspecies.size - 2, figsize=f_figsize)\n",
    "\n",
    "# Draw the first found bee of given subpecies\n",
    "i=0\n",
    "for s in subspecies[2:]:\n",
    "    if s == 'healthy': continue\n",
    "    file=img_folder + bees[bees['subspecies']==s].iloc[0]['file']\n",
    "    im=imageio.imread(file)\n",
    "    ax[i].imshow(im, resample=True)\n",
    "    ax[i].set_title(s, fontsize=8)\n",
    "    i+=1\n",
    "    \n",
    "plt.suptitle(\"Subspecies of Bee\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample some healthy objects to have a look at\n",
    "ncols = 5\n",
    "healthy = bees[bees['health'] == 'healthy'].sample(ncols)\n",
    "f, ax = plt.subplots(nrows=1,ncols=ncols, figsize=f_figsize)\n",
    "\n",
    "for i in range(0, ncols): \n",
    "    file = img_folder + healthy.iloc[i]['file']\n",
    "    ax[i].imshow(imageio.imread(file))\n",
    "\n",
    "plt.suptitle(\"Healthy Bees\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "health_cats = bees['health'].cat.categories\n",
    "f, ax = plt.subplots(1, health_cats.size-1, figsize=f_figsize)\n",
    "\n",
    "# Draw the first found bee with a particulat health issue\n",
    "i=0\n",
    "for c in health_cats:\n",
    "    if c == 'healthy': continue\n",
    "    bee = bees[bees['health'] == c].sample(1).iloc[0]\n",
    "    ax[i].imshow(imageio.imread(img_folder + bee['file']))\n",
    "    ax[i].set_title(bee['health'], fontsize=8)\n",
    "    i += 1\n",
    "    \n",
    "plt.suptitle(\"Sick Bees\")    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accorgin to the plots above labels are imbalanced, so let's fix it on our own. Splitting should be done before balancing to avoid putting the same upsampled Bee to both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_balance(bees, target):\n",
    "    # Split to train and test before balancing\n",
    "    train_bees, test_bees = train_test_split(bees, random_state=24)\n",
    "\n",
    "    # Split train to train and validation datasets\n",
    "    train_bees, val_bees = train_test_split(train_bees, test_size=0.1, random_state=24)\n",
    "\n",
    "    #Balance by subspecies to train_bees_bal_ss dataset\n",
    "    # Number of samples in each category\n",
    "    ncat_bal = int(len(train_bees)/train_bees[target].cat.categories.size)\n",
    "    train_bees_bal = train_bees.groupby(target, as_index=False).apply(lambda g:  g.sample(ncat_bal, replace=True)).reset_index(drop=True)\n",
    "    return(train_bees_bal, val_bees, test_bees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bees_bal, val_bees, test_bees = split_balance(bees, 'subspecies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2,1, figsize=(12,8), sharex = True)\n",
    "\n",
    "# Oridigan data\n",
    "ax = bees['subspecies'].value_counts().plot(kind='bar', rot = 0, ax=axs[0])\n",
    "ax.set_title('Original dataset')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Balanced train\n",
    "ax = train_bees_bal['subspecies'].value_counts().plot(kind='bar', rot = 0, ax=axs[1])\n",
    "ax.set_title('Train dataset after balancing')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspecies classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some default network parameters\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 100, 100\n",
    "KERNEL_SIZE = 3\n",
    "IMAGE_CHANNELS = 3\n",
    "RANDOM_STATE = 1337\n",
    "N_EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_POOL_DIM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find a few auxiliary functions that we will help us through the model-building procedure. The dataset contains images of different shapes. The function below helps us to read images from the image-files and scale all images to IMAGE_WIDTH x IMAGE_HEIGHT x IMAGE_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file, img_folder='bee_imgs/bee_imgs/'):    \n",
    "    img = skimage.io.imread(img_folder + file)\n",
    "    img = skimage.transform.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), mode='reflect')\n",
    "    return img[:,:,:IMAGE_CHANNELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to evaluate the model, estimating the training error and accuracy and also the validation error and accuracy. It may help us to decide how our workflow is going, e.g. if we have a high bias, we may firstly try to improve our model so that it will learn better the train images dataset. Otherwise, in case we have a small bias but large variance, that means our model faces overfitting. Based on these kind of observation, we make decission for how to adjust the model.\n",
    "\n",
    "`tf.keras.preprocessing.image.ImageDataGenerator` may help us to generate batches of tensor-image-data with [real-time data augmentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare2train(train_bees, val_bees, test_bees, target):\n",
    "    # Bees already splitted to train, validation and test\n",
    "    # Load and transform images to have equal width/height/channels. \n",
    "    # read_img function is defined to use in both health and subspecies. \n",
    "    # Use np.stack to get NumPy array for CNN input\n",
    "\n",
    "    # Train data\n",
    "    train_X = np.stack(train_bees['file'].apply(read_img))\n",
    "    train_y  = pd.get_dummies(train_bees[target], drop_first=False)\n",
    "\n",
    "    # Validation during training data to calculate val_loss metric\n",
    "    val_X = np.stack(val_bees['file'].apply(read_img))\n",
    "    val_y = pd.get_dummies(val_bees[target], drop_first=False)\n",
    "\n",
    "    # Test data\n",
    "    test_X = np.stack(test_bees['file'].apply(read_img))\n",
    "    test_y = pd.get_dummies(test_bees[target], drop_first=False)\n",
    "\n",
    "    # Data augmentation - a little bit rotate, zoom and shift input images.\n",
    "    generator = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.1, # Randomly zoom image \n",
    "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True)\n",
    "    generator.fit(train_X)\n",
    "    return (generator, train_X, val_X, test_X, train_y, val_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, train_X, val_X, test_X, train_y, val_y, test_y = prepare2train(train_bees_bal, val_bees, test_bees, 'subspecies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll stop training if no improvement after some epochs\n",
    "earlystopper1 = keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "\n",
    "# Save the best model during the traning\n",
    "checkpointer1 = keras.callbacks.ModelCheckpoint('best_model1.h5',\n",
    "                                                monitor='val_accuracy',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True)\n",
    "# Build CNN model\n",
    "model1=Sequential()\n",
    "model1.add(Conv2D(16, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))\n",
    "model1.add(MaxPool2D(MAX_POOL_DIM))\n",
    "model1.add(Conv2D(16, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(train_y.columns.size, activation='softmax'))\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the first model using `Model.fit_generator`, however it is possible to call `Model.fit`, which supports generators, in case of new release of TF, and a predefined batch size. Basically, `steps_per_epoch` may be increased as we use augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1 = model1.fit_generator(generator.flow(train_X,train_y, batch_size=BATCH_SIZE),\n",
    "                                 epochs=N_EPOCH,\n",
    "                                 validation_data=[val_X, val_y],\n",
    "                                 steps_per_epoch=50,\n",
    "                                 callbacks=[earlystopper1, checkpointer1])\n",
    "# Get the best saved weights\n",
    "model1.load_weights('best_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(training, model, test_X, test_y, target):\n",
    "    \n",
    "    ## Trained model analysis and evaluation\n",
    "    f, ax = plt.subplots(2,1, figsize=(5,5))\n",
    "    ax[0].plot(training.history['loss'], label=\"Loss\")\n",
    "    ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n",
    "    ax[0].set_title('%s: loss' % target)\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    ax[1].plot(training1.history['accuracy'], label=\"Accuracy\")\n",
    "    ax[1].plot(training1.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "    ax[1].set_title('%s: accuracy' % target)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy by subspecies\n",
    "    test_pred = model.predict(test_X)    \n",
    "    acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum()/test_y.sum()\n",
    "    acc_by_subspecies.plot(kind='bar', title='Accuracy by %s' % target)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Classification report\")\n",
    "    test_pred = np.argmax(test_pred, axis=1)\n",
    "    test_truth = np.argmax(test_y.values, axis=1)\n",
    "    print(classification_report(test_truth, test_pred, target_names=test_y.columns))\n",
    "\n",
    "    # Loss function and accuracy\n",
    "    test_res = model.evaluate(test_X, test_y.values, verbose=0)\n",
    "    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(training1, model1, test_X, test_y, 'subspecies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function below displays how input sample image looks like after you apply convolution using some particular kernel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_kernels(img, conv_layer, title):\n",
    "    # Extract kernels from given layer\n",
    "    weights1 = conv_layer.get_weights()\n",
    "    kernels = weights1[0]\n",
    "    kernels_num = kernels.shape[3]\n",
    "    \n",
    "    # Each row contains 3 images: kernel, input image, output image\n",
    "    f, ax = plt.subplots(kernels_num, 3, figsize=(10, kernels_num*4))\n",
    "\n",
    "    for i in range(0, kernels_num):\n",
    "        # Get kernel from the layer and draw it\n",
    "        kernel=kernels[:,:,:3,i]\n",
    "        ax[i][0].imshow((kernel * 255).astype(np.uint8), vmin=0, vmax=255)\n",
    "        ax[i][0].set_title(\"Kernel %d\" % i, fontsize = 9)\n",
    "        \n",
    "        # Get and draw sample image from test data\n",
    "        ax[i][1].imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n",
    "        ax[i][1].set_title(\"Before\", fontsize=8)\n",
    "        \n",
    "        # Filtered image - apply convolution\n",
    "        img_filt = convolve(img, kernel)\n",
    "        ax[i][2].imshow((img_filt * 255).astype(np.uint8), vmin=0, vmax=255)\n",
    "        ax[i][2].set_title(\"After\", fontsize=8)\n",
    "        \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample image to visualize convolution\n",
    "idx = np.random.randint(0,len(test_X)-1)\n",
    "img = test_X[idx,:,:,:]\n",
    "# Take 1st convolutional layer and look at it's filters\n",
    "conv1 = model1.layers[0]\n",
    "img = visualize_layer_kernels(img, conv1, \"Subspecies CNN. Layer 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample another image to visualize convolutoin\n",
    "idx = np.random.randint(0,len(test_y)-1)\n",
    "img = test_X[idx,:,:,:]\n",
    "# Take another convolutional layer and look at it's filters\n",
    "conv2 = model1.layers[2]\n",
    "res = visualize_layer_kernels(img, conv2, \"Subspecies CNN. Layer 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional data will only slightly increase the accuracy of the training set.\n",
    "To reduce the loss of the validation set (which is a sign of overfitting), we can have such strategies as:\n",
    "\n",
    "* add Dropout layers;\n",
    "* introduce strides;\n",
    "* modify the Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick up the first one. \n",
    "The Dropout layer randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - `rate`) such that the sum over all inputs is unchanged.\n",
    "\n",
    "Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to `True` automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(RANDOM_STATE)\n",
    "layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layer(data, training=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plug `Dropout` into the model. Should we add it as a last layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "#Use the same architecture as it was in model1 case, but add Dropout layers. (1 point)\n",
    "#You may increase the number of epoch to 10, but don't change other parameters and random state.\n",
    "#(2 point for best-performing shared architecture)\n",
    "#model2.add(...)\n",
    "<YOUR CODE>\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training2 = model2.fit_generator(generator.flow(train_X,train_y, batch_size=BATCH_SIZE),\n",
    "                                     epochs=N_EPOCH,\n",
    "                                     validation_data=[val_X, val_y],\n",
    "                                     steps_per_epoch=50,\n",
    "                                     callbacks=[earlystopper1, checkpointer1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(training2, model2, test_X, test_y, 'subspecies')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
