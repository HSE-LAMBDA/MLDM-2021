{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2021/blob/master/11-gans/MLDM_2021_seminar11_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcN9C9BKif-N"
   },
   "source": [
    "# GAN example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3WepHOYipBr"
   },
   "source": [
    "Today we'll try to generate people's faces.\n",
    "As always, let's start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j52S_cBVtLfS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw9cQTImimy8"
   },
   "source": [
    "And now we'll get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR-qOLLfQtta"
   },
   "outputs": [],
   "source": [
    "lfw = tfds.image_classification.LFW()\n",
    "lfw.download_and_prepare()\n",
    "ds = lfw.as_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzRfUNQgi3N4"
   },
   "source": [
    "Original images are a bit too large for this exercise - we want to keep it lightweight (although feel free to try different image sizes for the homework if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJuqE4BhRvYb"
   },
   "outputs": [],
   "source": [
    "def get_img(x):\n",
    "  return x['image'][80:-80,80:-80]\n",
    "\n",
    "data = np.array([\n",
    "  np.array(Image.fromarray(img.numpy()).resize((36, 36)))\n",
    "  for img in tqdm(ds['train'].map(get_img))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7jwFMkyjOxI"
   },
   "source": [
    "Let's have a look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9p3GwjbtWxr"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz8KQmNDumG7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NuBd7cunql"
   },
   "outputs": [],
   "source": [
    "plt.imshow(data[:25].reshape(5, 5, 36, 36, 3).transpose((0, 2, 1, 3, 4)).reshape(5 * 36, 5 * 36, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjgWLj3jjTHc"
   },
   "source": [
    "Ok, now let's build our GAN!\n",
    "\n",
    "First we'll preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLbXtftPxEC5"
   },
   "outputs": [],
   "source": [
    "X_train = data.astype('float32') / 255\n",
    "print(X_train.min(), X_train.max(), X_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBBdmTXVjYLh"
   },
   "source": [
    "Defining the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_kNbCPnwjh8"
   },
   "outputs": [],
   "source": [
    "ll = tf.keras.layers\n",
    "\n",
    "LATENT_DIM = 32\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "  ll.Dense(32, input_shape=(LATENT_DIM,), activation='relu'),\n",
    "  ll.Dense(64, activation='relu'),\n",
    "  ll.Dense(36 * 36 * 3, activation='sigmoid'),\n",
    "  ll.Reshape((36, 36, 3)),\n",
    "])\n",
    "\n",
    "discriminator = tf.keras.Sequential([\n",
    "  ll.Reshape((36 * 36 * 3,), input_shape=(36, 36, 3)),\n",
    "  ll.Dense(64, activation='relu'),\n",
    "  ll.Dropout(0.1),\n",
    "  ll.Dense(32, activation='relu'),\n",
    "  ll.Dropout(0.1),\n",
    "  ll.Dense(1),\n",
    "])\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nadyzCevj2ZP"
   },
   "source": [
    "Here we'll define our loss functions and optimization steps. Implement all the parts below. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gxjEFhdx886"
   },
   "outputs": [],
   "source": [
    "def gen_images(num):\n",
    "  return generator(tf.random.normal(shape=(num, LATENT_DIM)))\n",
    "\n",
    "# @tf.function decorator below compiles the function\n",
    "# it decorates into a static graph. This improves the performance\n",
    "# but there are some pitfalls one should be aware of when using it,\n",
    "# check out https://www.tensorflow.org/guide/function\n",
    "# for more details\n",
    "@tf.function\n",
    "def forward(batch):\n",
    "  real = batch\n",
    "  fake = gen_images(len(batch))\n",
    "\n",
    "  shape = (len(batch), 1)\n",
    "  labels_real = tf.ones (shape=shape)\n",
    "  ### Optional regularization technique:\n",
    "  ### set small amount of the 'real' labels\n",
    "  ### to being 'fake':\n",
    "  # labels_real = tf.cast(\n",
    "  #     tf.random.uniform(shape=shape) > 0.1,\n",
    "  #     'float32'\n",
    "  # )\n",
    "  labels_fake = tf.zeros(shape=shape)\n",
    "\n",
    "  X = tf.concat([real, fake], axis=0)\n",
    "  y = tf.concat([labels_real, labels_fake], axis=0)\n",
    "\n",
    "  # Note: it's important to call the discriminator with `training=True`\n",
    "  #       to make use of the Dropout layers.\n",
    "  loss = <YOUR CODE>\n",
    "  return loss\n",
    "\n",
    "opt_d = tf.optimizers.RMSprop()\n",
    "opt_g = tf.optimizers.RMSprop()\n",
    "\n",
    "@tf.function\n",
    "def disc_step(batch):\n",
    "  with tf.GradientTape() as t:\n",
    "    d_loss = <YOUR CODE>\n",
    "  grads = <YOUR CODE>\n",
    "  opt_d.apply_gradients(<YOUR CODE>)\n",
    "  return d_loss\n",
    "\n",
    "@tf.function\n",
    "def gen_step(batch):\n",
    "  with tf.GradientTape() as t:\n",
    "    g_loss = <YOUR CODE>\n",
    "  grads = <YOUR CODE>\n",
    "  opt_g.apply_gradients(<YOUR CODE>)\n",
    "  return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsb3o2tykmUB"
   },
   "source": [
    "Finally, let's write our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQKnxL0M0DfI"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZLclVcDzkFl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "N_EPOCHS = 25\n",
    "NUM_DISC_STEPS = 5\n",
    "\n",
    "i_disc_step = 0\n",
    "losses_gen = []\n",
    "losses_disc = []\n",
    "for i_ep in range(N_EPOCHS):\n",
    "  shuffle_ids = np.random.choice(len(X_train), len(X_train), replace=False)\n",
    "  epoch_loss_gen = []\n",
    "  epoch_loss_disc = []\n",
    "  for i_img in trange(0, len(X_train), BATCH_SIZE):\n",
    "    batch = X_train[shuffle_ids][i_img:i_img + BATCH_SIZE]\n",
    "\n",
    "    if i_disc_step < NUM_DISC_STEPS:\n",
    "      # discriminator update\n",
    "      i_disc_step += 1\n",
    "      epoch_loss_disc.append(disc_step(batch).numpy())\n",
    "    else:\n",
    "      # generator update\n",
    "      i_disc_step = 0\n",
    "      epoch_loss_gen.append(gen_step(batch).numpy())\n",
    "\n",
    "  losses_gen.append(np.mean(epoch_loss_gen))\n",
    "  losses_disc.append(np.mean(epoch_loss_disc))\n",
    "\n",
    "  opt_d.learning_rate.assign(opt_d.learning_rate * 0.99)\n",
    "  opt_g.learning_rate.assign(opt_g.learning_rate * 0.99)\n",
    "\n",
    "  imgs = (gen_images(25).numpy() * 255).astype('uint8')\n",
    "  clear_output(wait=True)\n",
    "  plt.figure(figsize=(12, 7))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(imgs.reshape((5, 5, 36, 36, 3)).transpose(0, 2, 1, 3, 4).reshape(36 * 5, 36 * 5, 3))\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(losses_gen, label='generator')\n",
    "  plt.plot(losses_disc, label='discriminator')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  print(\"Done with epoch #\", i_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lltag3hx77ha"
   },
   "source": [
    "# Interpolations in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot100(imgs):\n",
    "  plt.imshow(\n",
    "      np.array(imgs).reshape((10, 10, 36,36,3)).transpose(0, 2, 1, 3,4).reshape((360,360,3))\n",
    "  )\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix two noise-values, interpolate between them and generate objects, using these values as an input for the generator. Nearly the same we did for AE. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "\n",
    "# Fix some noise values with corresponding shapes:\n",
    "representation_1 = <YOUR CODE>\n",
    "representation_2 = <YOUR CODE>\n",
    "\n",
    "# Now create a matrix of linear interpolations between\n",
    "# the two representations:\n",
    "w = np.linspace(0, 1, 10)[None,:,None]\n",
    "representation_mixed = representation_1[:,None] * (1 - w) + representation_2[:,None] * w\n",
    "\n",
    "# Then generate the images from the mixed representations:\n",
    "mixed_imgs = <YOUR CODE>\n",
    "\n",
    "plt.figure(figsize=(6, 6), dpi=100)\n",
    "plot100(mixed_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple GAN that we built didn't let us control any parameters (e.g. hair color, gender) of the samples we generated. \n",
    "Let's pick up a classical dataset that we have alredy worked with - MNIST. In case we would create and fit GAN the same way, it wouldn't let us choose the class of digits we're generating. To be able to control what we generate, we need to somehow plug [*conditions*](https://arxiv.org/abs/1411.1784) into our model.\n",
    "\n",
    "In this example, we'll build a Conditional GAN that can generate MNIST handwritten digits conditioned on a given class. We'll create a simle FC-model just to cover some basics, but you may try to replace it with CNN model and play around some parameters like number of filters/layers, learning rates, etc, or check out [Keras Conditional GAN example](https://keras.io/examples/generative/conditional_gan/) that is used as a reference for the current notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and preprocess the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use all the available examples from both the training and test\n",
    "# sets.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "print(f\"Shape of training images: {all_digits.shape}\")\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a common unconditional GAN building procedure, we start by sampling noise (of some fixed dimension) from a normal distribution. In conditional situation, we also need to account for the class labels or other conditions that we have. Try to figure out the proper shape for the parameters bellow. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_in_channels = <YOUR CODE>\n",
    "discriminator_in_channels = <YOUR CODE>\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((discriminator_in_channels)),\n",
    "        layers.Reshape((image_size * image_size + discriminator_in_channels,)),\n",
    "        layers.Dense(32, activation=\"elu\"),\n",
    "        layers.Dense(32, activation=\"elu\"),\n",
    "        layers.Dense(16, activation=\"elu\"),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        layers.Dense(64, activation=\"elu\"),\n",
    "        layers.Dense(64, activation=\"elu\"),\n",
    "        layers.Dense(64, activation=\"elu\"),\n",
    "        layers.Dense(image_size * image_size),\n",
    "        layers.Reshape((image_size, image_size, num_channels)),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the class for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        #https://www.tensorflow.org/guide/extension_type\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def generate_images(self, one_hot_labels):\n",
    "        # Generate noise and concat it with the conditions\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(one_hot_labels.shape[0],latent_dim) )\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=-1\n",
    "        )\n",
    "        return self.generator(random_vector_labels)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :,]\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=-1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = tf.reshape(self.generator(random_vector_labels),(-1, image_size * image_size))\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([tf.reshape(real_images,(-1, image_size * image_size)), image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=-1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = tf.reshape(self.generator(random_vector_labels),(-1,28*28))\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that helps us to plot the samples during the fitting procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mn(images, m=10, n=10, shuffle=False):\n",
    "    if shuffle:\n",
    "        images = images[np.random.permutation(len(images))[:m * n]]\n",
    "    _, h, w, _ = images.shape\n",
    "    images = images[:m*n].reshape(m, n, *images.shape[1:])\n",
    "    images = images.transpose(0, 2, 1, 3, 4).reshape(m * h, n * w)\n",
    "    plt.imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `PlotImgsCallback` that is going to be triggered at the end of the epoch to plot samples using `plot_mn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotImgsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, period_in_epochs=1, clear=True):\n",
    "        super().__init__()\n",
    "        self.period_in_epochs = period_in_epochs\n",
    "        labels = np.tile(np.arange(10)[:,None], (1, 10)).reshape(100)\n",
    "        self.one_hot_labels = keras.utils.to_categorical(labels, 10)\n",
    "        self.clear = clear\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.clear:\n",
    "          clear_output(wait=True)\n",
    "        if (epoch + 1) % self.period_in_epochs == 0:\n",
    "            plt.figure(figsize=(7, 7))\n",
    "            plot_mn(\n",
    "                self.model.generate_images(\n",
    "                    self.one_hot_labels\n",
    "                ).numpy().clip(0, 1)\n",
    "            )\n",
    "            plt.title('Epoch = ' + str(epoch + 1))\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "# Instead of using NUM_DISC_STEPS parameter, we use different learning rates \n",
    "# for different optimizers.\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.0005),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=10, callbacks=[PlotImgsCallback(clear=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cond_gan.history.history['d_loss'])\n",
    "plt.plot(cond_gan.history.history['g_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['d_loss', 'g_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLDM_2021_seminar11_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
